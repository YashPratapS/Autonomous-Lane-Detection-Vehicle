# Autonomous-Lane-Detection-Vehicle
The purpose of this project is to create design and develop an autonomous car that can be remotely controlled while also having the ability to detect and avoid obstacles by utilizing Raspberry Pi and Open CV technology. The car is equipped with advanced sensors, which enable it to scan the surrounding environment and detect any potential obstacles. Additionally, the car is also equipped with a camera that captures a live video feed of the road, which is then analyzed by the Open CV library to identify and track lane markings.

![car (1)](https://github.com/YashPratapS/Autonomous-Lane-Detection-Vehicle/assets/95158391/7bc9b0b3-abda-4d1c-9fc6-87fff8a46a22)

# Materials Required:  
- Raspberry Pi (model 3 or higher)
- Pi Camera Module 
- Ultrasonic sensor (HC-SR04) 
- L298N motor driver 
- 3 DC Motors 
- Power bank 
- Chassis (3-wheel drive)

![image](https://github.com/YashPratapS/Autonomous-Lane-Detection-Vehicle/assets/95158391/cdf56f15-e041-4a41-aa47-eb49016530cb)

# Remotely Operated Vehicle:
After completing the assembly and connection parts we will be implementing various functionalities to our model which is divided into three parts of which operating the car remotely (Using a Computer) is first – We have used MobaXterm which is remote computing software and it allowed us to access the Raspbian operating system using IP address of the Raspberry Pi for wireless connectivity. We want to control the movement of the car using our computer i.e. the motion will be determined by the keys we press on the keyboard. So for that, we have made three Python files namely - motors.py, keyboard.py, and testdrive.py.

- Keyboard.py :
We want to read the impression from the keyboard as specific keys are assigned for specified motions. We have imported the Pygame [7] library to our code as it provides an easy-to-use interface to work with graphics, sound, and input devices such as keyboards and mice and helps to read the input that is provided from the keyboard. Our function repeatedly checks if a key is pressed or not and returns the assigned key as a result in the output. For example if ‘W’ is pressed then the output will depict W and so on.
- Motors.py :
After the key impression is received from the keyboard.py function, the next step is carried out by motors.py. We imported the GPIO library to our code as it will allow us to easily access the GPIO pins of a Raspberry Pi board. Now the GPIO pins are initialized for connection to the L298N motor module and raspberry pi. Now the servo motors of the car can be controlled using raspberry pi. We make a new file with the name motor.py for the movement of the car. Now when a command from the keyboard is passed to the motor.py file a function call will take place i.e. if W is pressed it is assigned to the forward motion function, S is assigned for backward motion, A  for steering the servo motors to the left and D for steering the servo motor to the right. The autonomous car is equipped with three servo motors, with two located at the front for controlling the forward and steering movements, and one at the back for controlling the backward movement and enabling the car to turn left or right.
- Test.py :
The last step is to command the car for motion and for that; we have made a new file named test.py. Here we have first imported keyboard.py and motors.py files. Now simply in this file, the output from the keyboard.py and motors.py files will be sensed and the required commands will follow. If W is pressed the car will move in the forward direction. If S is pressed the car will move in the backward direction. If A is pressed the car will steer in the left direction. If D is pressed the car will steer in the right direction hence we can simply operate our Vehicle with the computer.

# Obstacle Avoidance :
In the next phase of our project, we aim to make our car autonomous by equipping it with obstacle-avoidance capabilities and enabling wireless motion. To achieve this, we will be using the motors.py file from the previous phase, and also creating a new file called ultrasonic.py. The connections for the ultrasonic sensor were already established in the assembly phase, so we will focus on the programming aspect here. We will begin by importing the time library, which will be used later in the code. The ultrasonic sensor will be used to detect objects in the Vehicle's path. Ultrasonic sensors consist of two main components: the transmitter and the receiver. The transmitter emits sound waves, while the receiver detects the reflected waves. When the sound waves hit an object, they bounce back and are detected by the receiver. Ultrasonic sensors use high-frequency sound waves to detect the presence of objects or measure distances. They work on the principle of time-of-flight (TOF), where a transducer emits sound waves in a specific direction, usually in the range of 40 kHz. The sound waves travel through the air and bounce back when they encounter an object. The transducer then receives the reflected sound waves and converts them into electrical signals processed by the sensor's circuitry. The circuitry calculates the time it took for the sound waves to travel to the object and back, and based on this information, it determines the distance between the sensor and the object. The accuracy of ultrasonic sensors depends on various factors such as distance to the object, angle of incidence, the surface of the object, and temperature and humidity of the environment. Once we have obtained the distance of the obstacle from the car in the output, we will use it in our function. If the distance is less than or equal to our defined threshold distance, the car will stop, move backward, change its path to a new one, and then start moving forward again. This process will be performed recursively by the Vehicle automatically, allowing it to avoid any obstacles in its path. We have now designed a car that can move autonomously while avoiding obstacles in its path and can also be operated remotely.

![image](https://github.com/YashPratapS/Autonomous-Lane-Detection-Vehicle/assets/95158391/f5a94237-b249-4878-97c6-0e0a2b923986)

# Lane Detection :
In the final stage of our project, we aim to achieve complete autonomy for the car. The car will be able to navigate on its own by detecting the path and making necessary adjustments without any human intervention. So for this, we are using  Open CV [6] which is an open-source computer vision and machine learning software library, and we are also using the Numpy [8 ] library. Open CV offers a wide range of features including Image and video processing: Open CV is capable of processing images and videos by performing operations such as resizing, cropping, rotating, and filtering. Feature detection: Open CV can extract and identify various features from images and videos such as corners, edges, and blobs. These features can be used for various computer vision applications. Object recognition: Open CV can train machine learning models such as Haar cascades and Support Vector Machines (SVMs) for object recognition and detection. Tracking:  Open CV provides various algorithms such as Kalman filter and optical flow for real-time object tracking. Deep learning: Open CV supports deep learning frameworks such as TensorFlow and PyTorch, enabling developers to train and use neural networks for a wide range of computer vision tasks. A new file named Lane.py is created, where we import motor.py from our previous parts. In this part, we use a Pi camera module to capture live frames and apply various image processing techniques to these frames.

![image](https://github.com/YashPratapS/Autonomous-Lane-Detection-Vehicle/assets/95158391/1640592a-d9aa-44d2-b1ac-d0a6ccb57b15)

- Canny edge detection  :
Lane detection using Canny edge method involves the following steps: First we convert the colored images (BGR) obtained to Grayscale because in computer vision applications such as object detection, color is not always a crucial factor, and as a result, converting an image from BGR to grayscale can often be beneficial. Grayscale images require less data to be processed and are easier to analyze, which can lead to improved performance and simplified image processing algorithms. Thus, converting to grayscale can be an effective way to streamline image processing in computer vision applications where color is not necessary for accurate analysis.
- Apply a Gaussian blur :
A Gaussian blur is applied to the grayscale image to remove any noise that may be present in the image. This step helps to ensure that only the edges of the lane markings are detected.
- Apply Canny edge detection :
The Canny edge detection algorithm is applied to the blurred grayscale image. This algorithm detects the edges in the image by looking for areas of rapid changes in intensity. The output of this step is a binary image where the edges of the lane markings are represented by white pixels, and everything else is represented by black pixels.
- Apply a region of interest mask :
To isolate the lane markings from the rest of the image, a region of interest mask is applied to the binary image. This mask is a polygon defined based on the expected location and size of the lane markings. Detect lane lines: The final step is to use a line detection algorithm, such as the Hough transform, to detect the lines corresponding to the lane markings in the binary image. We will create a center line from the edges obtained from the Hough transform. The center line can have three possibilities of inclination with a horizontal axis- If the straight line is perpendicular to the horizontal axis the car will continue to move in the straight line as it was moving before. If the straight line subtends inclination to left, the car steers in the left direction, and if it subtends inclination towards right the car will steer to its right. Hence after implementing the third part of our project, we have successfully implemented an autonomous car that can be remotely operated, avoiding obstacles in its path and self-detecting the lanes.
